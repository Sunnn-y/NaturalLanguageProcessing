{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwiGlJmCBLuB3tiuejSq+j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sunnn-y/NaturalLanguageProcessing/blob/main/20231017_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D_%EC%8B%A4%EC%8A%B5(%EC%98%81%EB%AC%B8%2C_%ED%95%9C%EA%B8%80).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 영문전처리\n",
        "- NLTK(Natural Language Toolkit) 자연어처리 패키지를 사용하여\n",
        "토큰화 / 품사테깅 / 개체명인식 을 해봅시다\n"
      ],
      "metadata": {
        "id": "b-A9Ba9_kU2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 수집\n",
        "- 온라인 상에서 바로 데이터 수집해서 실습 : https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/?ss=ai-big-data&sh=72db15f11f25"
      ],
      "metadata": {
        "id": "E2_c7qRxZlwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "DjEWKYrvO73Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = requests.get('https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/?ss=ai-big-data&sh=72db15f11f25')\n",
        "bs = BeautifulSoup(res.text, \"html.parser\")"
      ],
      "metadata": {
        "id": "426wMyEDPJl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = bs.select('div.article-body > p')\n",
        "content = ''\n",
        "for i in data:\n",
        "  content += i.get_text()\n",
        "content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "0XgFdS_VPVk3",
        "outputId": "9efc5453-86ff-4978-bd2d-e3460853d8a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It is the present-day darling of the tech world. The current renaissance of Artificial Intelligence (AI) with its sister discipline Machine Learning (ML) has led every IT firm worth its salt to engineer some form of AI onto its platform, into its toolsets and throughout its software applications.And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.When did it all go so right?But AI used to be a fanciful notion mostly confined science fiction, so when did it all go right?In recent years we’ve had some big changes in technology. Aside from the proliferation of mobile devices that has impacted us all, memory has become a lot cheaper, data storage has become a lot easier (in cloud, and elsewhere) and computer processing speeds have continued to outstrip previous records. With the power of quantum computing around the corner, is the AI renaissance simply a result of the coming together of these ‘tech ingredient’ forces?So, in many ways, Dunning really heralds the modern era of the web as the key facilitator for the new age of AI. Information has become not just ubiquitous; it has also become easier to access and more accurately classified into structured, semi-structured and unstructured data in its rawest form.Tuning AI towards lifeDunning and MapR point out that the new generation of AI & ML is now rediscovering ideas, some of which were first thought of some 50 years ago. The difference today is, each time keep adding a bit of something new. A bit of computing power here, better data there, new ideas for organizing and optimizing a network and after a while we get to build new AI systems that really do useful work. So how should we continue to engineer these new systems?CEO of AI code analytics platform company Gamma is Vishal Rai. In general terms, Rai agrees that the AI renaissance has been driven by tectonic shifts in three areas in the computing world: computing power, swathes of data (and its accessibility)… but also by human ingenuity.He points to new developments coming out of both Silicon Valley but further afield also (China being a prime example, Huawei builds its smartphone chipsets around its Kirin AI-enriched microprocessor) and says that this is all helping to create future industries such as autonomous driving and health care diagnostics.Real world application of AI applicationsSo in what ways are the new real world applications of AI manifesting themselves and starting to impact the services we use below the surface?Cloud computing software intelligence and Application Performance Management (APM) specialist Dynatrace has now extended its AI-powered platform to include IBM Z mainframe support for CICS (a mainframe programming language), IMS (a mainframe database) and middleware. To put that in less technical terms, Dynatrace can be used to monitor software that sits on mainframes to make sure it stays healthy.Why bother? Because the mainframe was never built to be hammered by devices with massively busy data streams like mobile banking apps, games and other online niceties. This means we need AI to understand what impact the mainframe is having on the newer systems we build.AI as a work of artMany would argue that the path to contemporary AI has been a long slog, but the systems we build now keep finding clever shortcuts… so the momentum for the AI renaissance is actually building cumulatively.Some argue that AI never went away and that the current popularization of AI and its ensuing discussion is just a natural progression of a technology that simply needed to come through a period of adolescence. Either way, AI is in your smartphone and in your cloud computing services, so renaissance or not, let’s hope it continues to become a work of art.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 전처리(Text Preprocessing)"
      ],
      "metadata": {
        "id": "EjVpNaJrnVuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 토큰화(Tokenizing)\n",
        "- punkt tokenizer 참고 : https://www.nltk.org/_modules/nltk/tokenize/punkt.html"
      ],
      "metadata": {
        "id": "XCUF0sSSZpT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_tokenize() : 마침표와 구두점(온점(.), 반점(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호)으로 구분하여 토큰화"
      ],
      "metadata": {
        "id": "iV9GYRn0nEEM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtQdHBjEkJsl",
        "outputId": "1d8609f3-8744-465b-98d0-944911708750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "# word_tokenize\n",
        "# !pip install nltk\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk punkt tokenize download\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKA3WWVDloIW",
        "outputId": "775e0516-7b6e-48cd-a7fe-9f5a8b3940e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "token1 = word_tokenize(content)\n",
        "print(token1)"
      ],
      "metadata": {
        "id": "PFvudw_LmcPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded6bae7-1c7c-4b3d-a0b3-e5225630598a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'is', 'the', 'present-day', 'darling', 'of', 'the', 'tech', 'world', '.', 'The', 'current', 'renaissance', 'of', 'Artificial', 'Intelligence', '(', 'AI', ')', 'with', 'its', 'sister', 'discipline', 'Machine', 'Learning', '(', 'ML', ')', 'has', 'led', 'every', 'IT', 'firm', 'worth', 'its', 'salt', 'to', 'engineer', 'some', 'form', 'of', 'AI', 'onto', 'its', 'platform', ',', 'into', 'its', 'toolsets', 'and', 'throughout', 'its', 'software', 'applications.And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon.When', 'did', 'it', 'all', 'go', 'so', 'right', '?', 'But', 'AI', 'used', 'to', 'be', 'a', 'fanciful', 'notion', 'mostly', 'confined', 'science', 'fiction', ',', 'so', 'when', 'did', 'it', 'all', 'go', 'right', '?', 'In', 'recent', 'years', 'we', '’', 've', 'had', 'some', 'big', 'changes', 'in', 'technology', '.', 'Aside', 'from', 'the', 'proliferation', 'of', 'mobile', 'devices', 'that', 'has', 'impacted', 'us', 'all', ',', 'memory', 'has', 'become', 'a', 'lot', 'cheaper', ',', 'data', 'storage', 'has', 'become', 'a', 'lot', 'easier', '(', 'in', 'cloud', ',', 'and', 'elsewhere', ')', 'and', 'computer', 'processing', 'speeds', 'have', 'continued', 'to', 'outstrip', 'previous', 'records', '.', 'With', 'the', 'power', 'of', 'quantum', 'computing', 'around', 'the', 'corner', ',', 'is', 'the', 'AI', 'renaissance', 'simply', 'a', 'result', 'of', 'the', 'coming', 'together', 'of', 'these', '‘', 'tech', 'ingredient', '’', 'forces', '?', 'So', ',', 'in', 'many', 'ways', ',', 'Dunning', 'really', 'heralds', 'the', 'modern', 'era', 'of', 'the', 'web', 'as', 'the', 'key', 'facilitator', 'for', 'the', 'new', 'age', 'of', 'AI', '.', 'Information', 'has', 'become', 'not', 'just', 'ubiquitous', ';', 'it', 'has', 'also', 'become', 'easier', 'to', 'access', 'and', 'more', 'accurately', 'classified', 'into', 'structured', ',', 'semi-structured', 'and', 'unstructured', 'data', 'in', 'its', 'rawest', 'form.Tuning', 'AI', 'towards', 'lifeDunning', 'and', 'MapR', 'point', 'out', 'that', 'the', 'new', 'generation', 'of', 'AI', '&', 'ML', 'is', 'now', 'rediscovering', 'ideas', ',', 'some', 'of', 'which', 'were', 'first', 'thought', 'of', 'some', '50', 'years', 'ago', '.', 'The', 'difference', 'today', 'is', ',', 'each', 'time', 'keep', 'adding', 'a', 'bit', 'of', 'something', 'new', '.', 'A', 'bit', 'of', 'computing', 'power', 'here', ',', 'better', 'data', 'there', ',', 'new', 'ideas', 'for', 'organizing', 'and', 'optimizing', 'a', 'network', 'and', 'after', 'a', 'while', 'we', 'get', 'to', 'build', 'new', 'AI', 'systems', 'that', 'really', 'do', 'useful', 'work', '.', 'So', 'how', 'should', 'we', 'continue', 'to', 'engineer', 'these', 'new', 'systems', '?', 'CEO', 'of', 'AI', 'code', 'analytics', 'platform', 'company', 'Gamma', 'is', 'Vishal', 'Rai', '.', 'In', 'general', 'terms', ',', 'Rai', 'agrees', 'that', 'the', 'AI', 'renaissance', 'has', 'been', 'driven', 'by', 'tectonic', 'shifts', 'in', 'three', 'areas', 'in', 'the', 'computing', 'world', ':', 'computing', 'power', ',', 'swathes', 'of', 'data', '(', 'and', 'its', 'accessibility', ')', '…', 'but', 'also', 'by', 'human', 'ingenuity.He', 'points', 'to', 'new', 'developments', 'coming', 'out', 'of', 'both', 'Silicon', 'Valley', 'but', 'further', 'afield', 'also', '(', 'China', 'being', 'a', 'prime', 'example', ',', 'Huawei', 'builds', 'its', 'smartphone', 'chipsets', 'around', 'its', 'Kirin', 'AI-enriched', 'microprocessor', ')', 'and', 'says', 'that', 'this', 'is', 'all', 'helping', 'to', 'create', 'future', 'industries', 'such', 'as', 'autonomous', 'driving', 'and', 'health', 'care', 'diagnostics.Real', 'world', 'application', 'of', 'AI', 'applicationsSo', 'in', 'what', 'ways', 'are', 'the', 'new', 'real', 'world', 'applications', 'of', 'AI', 'manifesting', 'themselves', 'and', 'starting', 'to', 'impact', 'the', 'services', 'we', 'use', 'below', 'the', 'surface', '?', 'Cloud', 'computing', 'software', 'intelligence', 'and', 'Application', 'Performance', 'Management', '(', 'APM', ')', 'specialist', 'Dynatrace', 'has', 'now', 'extended', 'its', 'AI-powered', 'platform', 'to', 'include', 'IBM', 'Z', 'mainframe', 'support', 'for', 'CICS', '(', 'a', 'mainframe', 'programming', 'language', ')', ',', 'IMS', '(', 'a', 'mainframe', 'database', ')', 'and', 'middleware', '.', 'To', 'put', 'that', 'in', 'less', 'technical', 'terms', ',', 'Dynatrace', 'can', 'be', 'used', 'to', 'monitor', 'software', 'that', 'sits', 'on', 'mainframes', 'to', 'make', 'sure', 'it', 'stays', 'healthy.Why', 'bother', '?', 'Because', 'the', 'mainframe', 'was', 'never', 'built', 'to', 'be', 'hammered', 'by', 'devices', 'with', 'massively', 'busy', 'data', 'streams', 'like', 'mobile', 'banking', 'apps', ',', 'games', 'and', 'other', 'online', 'niceties', '.', 'This', 'means', 'we', 'need', 'AI', 'to', 'understand', 'what', 'impact', 'the', 'mainframe', 'is', 'having', 'on', 'the', 'newer', 'systems', 'we', 'build.AI', 'as', 'a', 'work', 'of', 'artMany', 'would', 'argue', 'that', 'the', 'path', 'to', 'contemporary', 'AI', 'has', 'been', 'a', 'long', 'slog', ',', 'but', 'the', 'systems', 'we', 'build', 'now', 'keep', 'finding', 'clever', 'shortcuts…', 'so', 'the', 'momentum', 'for', 'the', 'AI', 'renaissance', 'is', 'actually', 'building', 'cumulatively.Some', 'argue', 'that', 'AI', 'never', 'went', 'away', 'and', 'that', 'the', 'current', 'popularization', 'of', 'AI', 'and', 'its', 'ensuing', 'discussion', 'is', 'just', 'a', 'natural', 'progression', 'of', 'a', 'technology', 'that', 'simply', 'needed', 'to', 'come', 'through', 'a', 'period', 'of', 'adolescence', '.', 'Either', 'way', ',', 'AI', 'is', 'in', 'your', 'smartphone', 'and', 'in', 'your', 'cloud', 'computing', 'services', ',', 'so', 'renaissance', 'or', 'not', ',', 'let', '’', 's', 'hope', 'it', 'continues', 'to', 'become', 'a', 'work', 'of', 'art', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordPunctTokenizer() : 알파벳이 아닌 문자를 구분하여 토큰화"
      ],
      "metadata": {
        "id": "gyQbD0lJmOHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WordPunctTokenizer() : 알파벳이 아닌 문자를 구분하여 토큰화\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "token2 = WordPunctTokenizer().tokenize(content)\n",
        "print(token2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKpV0sY6UlPH",
        "outputId": "9b475ca0-adbf-4a49-9a08-12632b88626e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'is', 'the', 'present', '-', 'day', 'darling', 'of', 'the', 'tech', 'world', '.', 'The', 'current', 'renaissance', 'of', 'Artificial', 'Intelligence', '(', 'AI', ')', 'with', 'its', 'sister', 'discipline', 'Machine', 'Learning', '(', 'ML', ')', 'has', 'led', 'every', 'IT', 'firm', 'worth', 'its', 'salt', 'to', 'engineer', 'some', 'form', 'of', 'AI', 'onto', 'its', 'platform', ',', 'into', 'its', 'toolsets', 'and', 'throughout', 'its', 'software', 'applications', '.', 'And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'\", 's', 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i', '.', 'e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.', 'When', 'did', 'it', 'all', 'go', 'so', 'right', '?', 'But', 'AI', 'used', 'to', 'be', 'a', 'fanciful', 'notion', 'mostly', 'confined', 'science', 'fiction', ',', 'so', 'when', 'did', 'it', 'all', 'go', 'right', '?', 'In', 'recent', 'years', 'we', '’', 've', 'had', 'some', 'big', 'changes', 'in', 'technology', '.', 'Aside', 'from', 'the', 'proliferation', 'of', 'mobile', 'devices', 'that', 'has', 'impacted', 'us', 'all', ',', 'memory', 'has', 'become', 'a', 'lot', 'cheaper', ',', 'data', 'storage', 'has', 'become', 'a', 'lot', 'easier', '(', 'in', 'cloud', ',', 'and', 'elsewhere', ')', 'and', 'computer', 'processing', 'speeds', 'have', 'continued', 'to', 'outstrip', 'previous', 'records', '.', 'With', 'the', 'power', 'of', 'quantum', 'computing', 'around', 'the', 'corner', ',', 'is', 'the', 'AI', 'renaissance', 'simply', 'a', 'result', 'of', 'the', 'coming', 'together', 'of', 'these', '‘', 'tech', 'ingredient', '’', 'forces', '?', 'So', ',', 'in', 'many', 'ways', ',', 'Dunning', 'really', 'heralds', 'the', 'modern', 'era', 'of', 'the', 'web', 'as', 'the', 'key', 'facilitator', 'for', 'the', 'new', 'age', 'of', 'AI', '.', 'Information', 'has', 'become', 'not', 'just', 'ubiquitous', ';', 'it', 'has', 'also', 'become', 'easier', 'to', 'access', 'and', 'more', 'accurately', 'classified', 'into', 'structured', ',', 'semi', '-', 'structured', 'and', 'unstructured', 'data', 'in', 'its', 'rawest', 'form', '.', 'Tuning', 'AI', 'towards', 'lifeDunning', 'and', 'MapR', 'point', 'out', 'that', 'the', 'new', 'generation', 'of', 'AI', '&', 'ML', 'is', 'now', 'rediscovering', 'ideas', ',', 'some', 'of', 'which', 'were', 'first', 'thought', 'of', 'some', '50', 'years', 'ago', '.', 'The', 'difference', 'today', 'is', ',', 'each', 'time', 'keep', 'adding', 'a', 'bit', 'of', 'something', 'new', '.', 'A', 'bit', 'of', 'computing', 'power', 'here', ',', 'better', 'data', 'there', ',', 'new', 'ideas', 'for', 'organizing', 'and', 'optimizing', 'a', 'network', 'and', 'after', 'a', 'while', 'we', 'get', 'to', 'build', 'new', 'AI', 'systems', 'that', 'really', 'do', 'useful', 'work', '.', 'So', 'how', 'should', 'we', 'continue', 'to', 'engineer', 'these', 'new', 'systems', '?', 'CEO', 'of', 'AI', 'code', 'analytics', 'platform', 'company', 'Gamma', 'is', 'Vishal', 'Rai', '.', 'In', 'general', 'terms', ',', 'Rai', 'agrees', 'that', 'the', 'AI', 'renaissance', 'has', 'been', 'driven', 'by', 'tectonic', 'shifts', 'in', 'three', 'areas', 'in', 'the', 'computing', 'world', ':', 'computing', 'power', ',', 'swathes', 'of', 'data', '(', 'and', 'its', 'accessibility', ')…', 'but', 'also', 'by', 'human', 'ingenuity', '.', 'He', 'points', 'to', 'new', 'developments', 'coming', 'out', 'of', 'both', 'Silicon', 'Valley', 'but', 'further', 'afield', 'also', '(', 'China', 'being', 'a', 'prime', 'example', ',', 'Huawei', 'builds', 'its', 'smartphone', 'chipsets', 'around', 'its', 'Kirin', 'AI', '-', 'enriched', 'microprocessor', ')', 'and', 'says', 'that', 'this', 'is', 'all', 'helping', 'to', 'create', 'future', 'industries', 'such', 'as', 'autonomous', 'driving', 'and', 'health', 'care', 'diagnostics', '.', 'Real', 'world', 'application', 'of', 'AI', 'applicationsSo', 'in', 'what', 'ways', 'are', 'the', 'new', 'real', 'world', 'applications', 'of', 'AI', 'manifesting', 'themselves', 'and', 'starting', 'to', 'impact', 'the', 'services', 'we', 'use', 'below', 'the', 'surface', '?', 'Cloud', 'computing', 'software', 'intelligence', 'and', 'Application', 'Performance', 'Management', '(', 'APM', ')', 'specialist', 'Dynatrace', 'has', 'now', 'extended', 'its', 'AI', '-', 'powered', 'platform', 'to', 'include', 'IBM', 'Z', 'mainframe', 'support', 'for', 'CICS', '(', 'a', 'mainframe', 'programming', 'language', '),', 'IMS', '(', 'a', 'mainframe', 'database', ')', 'and', 'middleware', '.', 'To', 'put', 'that', 'in', 'less', 'technical', 'terms', ',', 'Dynatrace', 'can', 'be', 'used', 'to', 'monitor', 'software', 'that', 'sits', 'on', 'mainframes', 'to', 'make', 'sure', 'it', 'stays', 'healthy', '.', 'Why', 'bother', '?', 'Because', 'the', 'mainframe', 'was', 'never', 'built', 'to', 'be', 'hammered', 'by', 'devices', 'with', 'massively', 'busy', 'data', 'streams', 'like', 'mobile', 'banking', 'apps', ',', 'games', 'and', 'other', 'online', 'niceties', '.', 'This', 'means', 'we', 'need', 'AI', 'to', 'understand', 'what', 'impact', 'the', 'mainframe', 'is', 'having', 'on', 'the', 'newer', 'systems', 'we', 'build', '.', 'AI', 'as', 'a', 'work', 'of', 'artMany', 'would', 'argue', 'that', 'the', 'path', 'to', 'contemporary', 'AI', 'has', 'been', 'a', 'long', 'slog', ',', 'but', 'the', 'systems', 'we', 'build', 'now', 'keep', 'finding', 'clever', 'shortcuts', '…', 'so', 'the', 'momentum', 'for', 'the', 'AI', 'renaissance', 'is', 'actually', 'building', 'cumulatively', '.', 'Some', 'argue', 'that', 'AI', 'never', 'went', 'away', 'and', 'that', 'the', 'current', 'popularization', 'of', 'AI', 'and', 'its', 'ensuing', 'discussion', 'is', 'just', 'a', 'natural', 'progression', 'of', 'a', 'technology', 'that', 'simply', 'needed', 'to', 'come', 'through', 'a', 'period', 'of', 'adolescence', '.', 'Either', 'way', ',', 'AI', 'is', 'in', 'your', 'smartphone', 'and', 'in', 'your', 'cloud', 'computing', 'services', ',', 'so', 'renaissance', 'or', 'not', ',', 'let', '’', 's', 'hope', 'it', 'continues', 'to', 'become', 'a', 'work', 'of', 'art', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TreebankWordTokenizer() : 정규표현식에 기반한 토큰화"
      ],
      "metadata": {
        "id": "E5klVk-dmIYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TreebankWordTokenizer() : 정규표현식에 기반한 토큰화\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "token = TreebankWordTokenizer().tokenize(content)\n",
        "print(token[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJP4OBTKUlMz",
        "outputId": "7b3c8737-3c2c-4b5d-aceb-cfb45bdf4cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'is', 'the', 'present-day', 'darling', 'of', 'the', 'tech', 'world.', 'The', 'current', 'renaissance', 'of', 'Artificial', 'Intelligence', '(', 'AI', ')', 'with', 'its']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 품사 부착(PoS Tagging)\n",
        "- 분리한 토큰마다 품사를 부착\n",
        "- 참고 : https://www.nltk.org/api/nltk.tag.html\n",
        "- 태그 목록 : https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
      ],
      "metadata": {
        "id": "aQeYxbf9ZgpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgU1TJ87UlJ_",
        "outputId": "bd35179c-6039-41dd-ac3a-6558b1996335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "taggedToken = pos_tag(token1)\n",
        "print(taggedToken[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbZWoYiVUlHM",
        "outputId": "ead7555f-0a7a-4b40-cfe3-996b54428d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('It', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('present-day', 'JJ'), ('darling', 'NN'), ('of', 'IN'), ('the', 'DT'), ('tech', 'JJ'), ('world', 'NN'), ('.', '.'), ('The', 'DT'), ('current', 'JJ'), ('renaissance', 'NN'), ('of', 'IN'), ('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('(', '('), ('AI', 'NNP'), (')', ')'), ('with', 'IN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 개체명인식(NER)\n",
        "- 참고 : https://www.nltk.org/api/nltk.chunk.html"
      ],
      "metadata": {
        "id": "JzRiGWdJZXx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예문 : Barack Obama likes fried chicken very much\n",
        "# word_tokenize() : 마침표와 구두점(온점(.), 반점(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호)으로 구분하여 토큰화\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX6zO8frWte_",
        "outputId": "ff77a942-ec14-4ed0-c244-bb4094acd684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "# 토큰화\n",
        "token1 = word_tokenize('Barack Obama likes fried chicken very much')\n",
        "print('token1 :', token1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy6pf1P0WtcK",
        "outputId": "5bb62fad-39ea-4926-dd9d-66f75282a1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token1 : ['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pos-tag\n",
        "taggedToken = pos_tag(token1)\n",
        "print('pos-tag :', taggedToken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aThjFZ2WtZi",
        "outputId": "dcd7fb86-b9a4-4292-c945-4981615de77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos-tag : [('Barack', 'NNP'), ('Obama', 'NNP'), ('likes', 'VBZ'), ('fried', 'VBN'), ('chicken', 'JJ'), ('very', 'RB'), ('much', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 개체명 인식\n",
        "from nltk import ne_chunk\n",
        "neToken = ne_chunk(taggedToken)\n",
        "print(neToken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xeQEnDRW0Gs",
        "outputId": "e4617d27-0362-4758-b90b-81108de0b13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (ORGANIZATION Obama/NNP)\n",
            "  likes/VBZ\n",
            "  fried/VBN\n",
            "  chicken/JJ\n",
            "  very/RB\n",
            "  much/JJ)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한글전처리\n",
        "- KoNLPy 한국어 자연어처리 패키지를 사용하여\n",
        "토큰화 / 품사테깅 / 불용어처리 를 해봅시다"
      ],
      "metadata": {
        "id": "R6Lv0FTukcV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 전처리(Text Preprocessing)\n",
        "- 텍스트를 자연어 처리를 위해 용도에 맞도록 사전에 표준화하는 작업\n",
        "- 텍스트 내 정보를 유지하고, 중복을 제거하여 분석 효율성을 높이기 위해 전처리를 수행\n"
      ],
      "metadata": {
        "id": "TUOlbx7Jb6Te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 토큰화(Tokenizing)\n",
        "- 텍스트를 자연어 처리를 위해 분리하는 것\n",
        "- 토큰화는 단어별로 분리하는 \"단어 토큰화(Word Tokenizing)\"와 문장별로 분리하는 \"문장 토큰화(Sentence Tokenizing)\"로 구분"
      ],
      "metadata": {
        "id": "qttj2FeAfQ7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''인생은 모두가 함께하는 여행이다. 매일매일 사는 동안 우리가 할 수 있는 건 최선을 다해 이 멋진 여행을 만끽하는 것이다.'''"
      ],
      "metadata": {
        "id": "U71VvtZNkg28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 띄어쓰기로 토큰화\n",
        "print(text.split(' '))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znH9Mep_bZwa",
        "outputId": "07fe8ced-d5e8-49ef-cc3d-838f183ac042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인생은', '모두가', '함께하는', '여행이다.', '매일매일', '사는', '동안', '우리가', '할', '수', '있는', '건', '최선을', '다해', '이', '멋진', '여행을', '만끽하는', '것이다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlb36VK6bZt1",
        "outputId": "a5ba7e5a-5383-4c6f-cdd7-260a7e109ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코모란\n",
        "from konlpy.tag import Komoran\n",
        "# 선언\n",
        "komoran = Komoran()\n",
        "# 토큰화 : morphs\n",
        "komoran_tokens = komoran.morphs(text)\n",
        "print(komoran_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pOTxZhobZra",
        "outputId": "81055a4f-7912-4ebc-ba77-47c9e94397c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인생', '은', '모두', '가', '함께', '하', '는', '여행', '이', '다', '.', '매일', '매일', '살', '는', '동안', '우리', '가', '하', 'ㄹ', '수', '있', '는', '건', '최선', '을', '다', '하', '아', '이', '멋지', 'ㄴ', '여행', '을', '만끽', '하', '는', '것', '이', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한나눔\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum = Hannanum()\n",
        "hannanum_tokens = hannanum.morphs(text)\n",
        "print(hannanum_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCXJulfcbZmo",
        "outputId": "61bc59ff-c21e-4e28-d6b1-42c3aba7a6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인생', '은', '모두', '가', '함께하', '는', '여행', '이', '다', '.', '매일매일', '사', '는', '동안', '우리', '가', '하', 'ㄹ', '수', '있', '는', '거', '은', '최선', '을', '다하', '어', '이', '멋지', 'ㄴ', '여행', '을', '만끽', '하', '는', '것', '이', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Okt : 형태소분석기를 만들기 위해 학습된 데이터가 SNS data(Twitter, Instagram 등)인 패키지이다.\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "okt_tokens = okt.morphs(text)\n",
        "print(okt_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02Gbfn_dcQBM",
        "outputId": "060956a5-ac71-406a-9772-2ca84edee03b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인생', '은', '모두', '가', '함께', '하는', '여행', '이다', '.', '매', '일', '매일', '사는', '동안', '우리', '가', '할', '수', '있는', '건', '최선', '을', '다해', '이', '멋진', '여행', '을', '만끽', '하는', '것', '이다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kkma : 가장 세밀한 단위로 형태소 분석함\n",
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "kkma_tokens = kkma.morphs(text)\n",
        "print(kkma_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeZoQ85jcP-u",
        "outputId": "1018f212-5d00-4064-c6dd-898e243f17bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인생', '은', '모두', '가', '함께', '하', '는', '여행', '이', '다', '.', '매일', '매일', '살', '는', '동안', '우리', '가', '하', 'ㄹ', '수', '있', '는', '것', '은', '최선', '을', '다하', '어', '이', '멋지', 'ㄴ', '여행', '을', '만끽', '하', '는', '것', '이', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 품사 부착(PoS Tagging)\n",
        "- 각 토큰에 품사 정보를 부착\n",
        "- 분석 시에 불필요한 품사를 제거하거나(예. 조사, 접속사 등) 필요한 품사를 필터링하기 위해 사용"
      ],
      "metadata": {
        "id": "T7rxrME2eDmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 코모란\n",
        "komoranTag = []\n",
        "for token in komoran_tokens:\n",
        "  komoranTag += komoran.pos(token) # komoran.pos(text) : 형태소분석 + 품사태깅을 동시에 할 수 있다\n",
        "\n",
        "print(komoranTag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hbclJJ_cP76",
        "outputId": "a2d56810-5d80-4281-b109-be0631ca7041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인생', 'NNG'), ('은', 'NNP'), ('모두', 'MAG'), ('가', 'VV'), ('아', 'EC'), ('함께', 'MAG'), ('하', 'NNG'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('여행', 'NNG'), ('이', 'MM'), ('다', 'MAG'), ('.', 'SF'), ('매일', 'MAG'), ('매일', 'MAG'), ('살', 'VV'), ('ㄹ', 'ETM'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('동안', 'NNG'), ('우리', 'NP'), ('가', 'VV'), ('아', 'EC'), ('하', 'NNG'), ('ㄹ', 'NA'), ('수', 'NNB'), ('있', 'VV'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('건', 'NNB'), ('최선', 'NNP'), ('을', 'NNG'), ('다', 'MAG'), ('하', 'NNG'), ('아', 'IC'), ('이', 'MM'), ('멋', 'NNG'), ('지', 'NNB'), ('ㄴ', 'JX'), ('여행', 'NNG'), ('을', 'NNG'), ('만끽', 'NNP'), ('하', 'NNG'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('것', 'NNB'), ('이', 'MM'), ('다', 'MAG'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한나눔\n",
        "hannanumTag = []\n",
        "for token in hannanum_tokens:\n",
        "  hannanumTag += hannanum.pos(token)  # hannanum.pos(text) : 형태소분석 + 품사태깅\n",
        "\n",
        "print(hannanumTag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7BjTk9EcP5X",
        "outputId": "bf8fc230-20c3-417b-a9b5-f05639ad3911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인생', 'N'), ('은', 'N'), ('모두', 'M'), ('가', 'J'), ('함께하', 'P'), ('어', 'E'), ('늘', 'P'), ('ㄴ', 'E'), ('여행', 'N'), ('이', 'M'), ('다', 'M'), ('.', 'S'), ('매일매일', 'M'), ('사', 'N'), ('늘', 'P'), ('ㄴ', 'E'), ('동안', 'N'), ('우리', 'N'), ('가', 'J'), ('하', 'I'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('늘', 'P'), ('ㄴ', 'E'), ('것', 'N'), ('은', 'N'), ('최선', 'N'), ('을', 'N'), ('다하', 'P'), ('어', 'E'), ('어', 'N'), ('이', 'M'), ('멋지', 'N'), ('ㄴ', 'N'), ('여행', 'N'), ('을', 'N'), ('만끽', 'N'), ('하', 'I'), ('늘', 'P'), ('ㄴ', 'E'), ('것', 'N'), ('이', 'M'), ('다', 'M'), ('.', 'S')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Okt\n",
        "oktTag = []\n",
        "for token in okt_tokens:\n",
        "  oktTag += okt.pos(token)  # okt.pos(text) : 형태소분석 + 품사태깅\n",
        "\n",
        "print(oktTag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVHiPM2NcP2y",
        "outputId": "6a5eabf7-776f-41be-fec7-a86ee2d16564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인생', 'Noun'), ('은', 'Noun'), ('모두', 'Noun'), ('가', 'Verb'), ('함께', 'Adverb'), ('하는', 'Verb'), ('여행', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation'), ('매', 'Noun'), ('일', 'Noun'), ('매일', 'Noun'), ('사는', 'Verb'), ('동안', 'Noun'), ('우리', 'Noun'), ('가', 'Verb'), ('할', 'Verb'), ('수', 'Noun'), ('있는', 'Adjective'), ('건', 'Noun'), ('최선', 'Noun'), ('을', 'Josa'), ('다해', 'Noun'), ('이', 'Noun'), ('멋진', 'Adjective'), ('여행', 'Noun'), ('을', 'Josa'), ('만끽', 'Noun'), ('하는', 'Verb'), ('것', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kkma\n",
        "kkmaTag = []\n",
        "for token in kkma_tokens:\n",
        "  kkmaTag += kkma.pos(token)  # kkma.pos(text) : 형태소분석 + 품사태깅\n",
        "\n",
        "print(kkmaTag)"
      ],
      "metadata": {
        "id": "iJO_hn6Se15X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 불용어 처리(Stopword)\n",
        "- 자연어 처리를 위해 불필요한 요소를 제거하는 작업\n",
        "- 불필요한 품사를 제거하는 작업과 불필요한 단어를 제거하는 작업으로 구성\n",
        "- 불필요한 토큰을 제거함으로써 연산의 효율성을 높임"
      ],
      "metadata": {
        "id": "p263IF6ofKjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Okt\n",
        "# 최빈어 조회. 최빈어를 조회하여 불용어 제거 대상을 선정\n",
        "from collections import Counter\n",
        "Counter(oktTag).most_common()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30vLyHk8fNtI",
        "outputId": "0935f50d-eb48-4b9c-c8cb-11698ec2c7b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('가', 'Verb'), 2),\n",
              " (('하는', 'Verb'), 2),\n",
              " (('여행', 'Noun'), 2),\n",
              " (('이다', 'Josa'), 2),\n",
              " (('.', 'Punctuation'), 2),\n",
              " (('을', 'Josa'), 2),\n",
              " (('인생', 'Noun'), 1),\n",
              " (('은', 'Noun'), 1),\n",
              " (('모두', 'Noun'), 1),\n",
              " (('함께', 'Adverb'), 1),\n",
              " (('매', 'Noun'), 1),\n",
              " (('일', 'Noun'), 1),\n",
              " (('매일', 'Noun'), 1),\n",
              " (('사는', 'Verb'), 1),\n",
              " (('동안', 'Noun'), 1),\n",
              " (('우리', 'Noun'), 1),\n",
              " (('할', 'Verb'), 1),\n",
              " (('수', 'Noun'), 1),\n",
              " (('있는', 'Adjective'), 1),\n",
              " (('건', 'Noun'), 1),\n",
              " (('최선', 'Noun'), 1),\n",
              " (('다해', 'Noun'), 1),\n",
              " (('이', 'Noun'), 1),\n",
              " (('멋진', 'Adjective'), 1),\n",
              " (('만끽', 'Noun'), 1),\n",
              " (('것', 'Noun'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어 처리\n",
        "stopPos = ['Josa', 'Punctuation', 'Suffix', 'Alpha', 'Foreign', 'Unknown', 'Number']\n",
        "stopWord = ['을', '은', '가']\n",
        "\n",
        "word = []\n",
        "for tag in oktTag:\n",
        "  if tag[1] not in stopPos:\n",
        "    if tag[0] not in stopWord:\n",
        "      word.append(tag[0])\n",
        "\n",
        "print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEmVewBBfS_L",
        "outputId": "88ff032d-c247-44c8-ad90-c548ee6d4db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인생', '모두', '함께', '하는', '여행', '매', '일', '매일', '사는', '동안', '우리', '할', '수', '있는', '건', '최선', '다해', '이', '멋진', '여행', '만끽', '하는', '것']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글은 공식적인 개체명인식 패키지가 없다. github에서 다른 사람이 만든거 따와야 함"
      ],
      "metadata": {
        "id": "QNBW1FzRjPyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lWJ5R2ZtjYip"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}